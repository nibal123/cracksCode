import sys
import matplotlib
import shutil 
import os
import tensorflow as tf

def convert_from_keras_to_savedmodel(input_filename, export_path):
    tf.keras.backend.clear_session()
    tf.keras.backend.set_learning_phase(0)
    model = tf.keras.models.load_model(input_filename)

    if os.path.exists(export_path):
        shutil.rmtree(export_path)

    # Fetch the Keras session and save the model
    with tf.keras.backend.get_session() as sess:
        tf.saved_model.simple_save(
            sess,
            export_path,
            inputs={'input_image': model.input},
            outputs={t.name:t for t in model.outputs})
import os
import glob
import matplotlib.pyplot as plt
from IPython.display import SVG
from tensorflow.python.keras.callbacks import TensorBoard
from tensorflow.python.keras.utils import plot_model
from tensorflow.python.keras.preprocessing.image import ImageDataGenerator
from tensorflow.python.keras.callbacks import CSVLogger
from tensorflow.python.keras.callbacks import ModelCheckpoint
from tensorflow.python.keras.optimizers import SGD
from tensorflow.python.keras.regularizers import l2
from tensorflow.python.keras.layers.advanced_activations import ELU
from tensorflow.python.layers.core import Dense
from tensorflow.python.layers.core import Dropout
from tensorflow.python.layers.core import Flatten
from tensorflow.python.layers.pooling import MaxPooling2D
from tensorflow.python.layers.normalization import BatchNormalization
from tensorflow.python.layers.convolutional import Conv2D
from tensorflow.python.keras.models import Sequential
import tensorflow as tf
# Parent directory containing src, checkpoints, models, etc.
WORKSPACE_BASE_PATH = "/home/s1171439/test1/tensorflow-crack-classification"
# Path were components are stored.
CODE_BASE_PATH = "/home/s1171439/test1/tensorflow-crack-classification/src/"
# Directory with data in case it is not inside WORKSPACE BASE path.
DATA_BASE_PATH = "/home/s1171439/test1/tensorflow-crack-classification/data/"
sys.path.append(CODE_BASE_PATH)


def build_cracknet(input_shape=(120, 120, 3), n_output_classes=2):
    model = Sequential()

    #bias_regularization = 0.0001
    #kernel_regularization = 0.0001

    # Convolution + Batch Norm. + ELU + Pooling #1
    model.add(
        Conv2D(32, (11, 11),
               input_shape=input_shape,
               activation='relu',
               strides=(1, 1),
               name="Conv1"
               # kernel_regularizer=l2(kernel_regularization),
               # bias_regularizer=l2(bias_regularization)
               ))
    model.add(BatchNormalization(name="Conv1BN"))
    model.add(ELU(name="Conv1ELU"))
    model.add(MaxPooling2D(pool_size=(7, 7), strides=(2, 2), name="Conv1Pool"))

    # Convolution + Batch Norm. + ELU + Pooling #2
    model.add(
        Conv2D(48, (11, 11),
               input_shape=(52, 52, 32),
               activation='relu',
               strides=(1, 1), name="Conv2"
               # kernel_regularizer=l2(kernel_regularization),
               # bias_regularizer=l2(bias_regularization)
               ))
    model.add(BatchNormalization(name="Conv2BN"))
    model.add(ELU(name="Conv2ELU"))
    model.add(MaxPooling2D(pool_size=(5, 5), strides=(2, 2), name="Conv2Pool"))

    # Convolution + Batch Norm. + ELU + Pooling #3
    model.add(
        Conv2D(64, (7, 7),
               input_shape=(19, 19, 48, 96),
               activation='relu',
               strides=(1, 1),
               name="Conv3"
               # kernel_regularizer=l2(kernel_regularization),
               # bias_regularizer=l2(bias_regularization)
               ))
    model.add(BatchNormalization(name="Conv3BN"))
    model.add(ELU(name="Conv3ELU"))
    model.add(MaxPooling2D(pool_size=(3, 3), strides=(1, 1), name="Conv3Pool"))

    # Convolution + Batch Norm. + ELU + Pooling #4
    model.add(
        Conv2D(80, (5, 5),
               activation='relu',
               strides=(1, 1),
               name="Conv4"
               # kernel_regularizer=l2(kernel_regularization),
               # bias_regularizer=l2(bias_regularization
               )
    )
    model.add(BatchNormalization(name="Conv4BN"))
    model.add(ELU(name="Conv4ELU"))
    model.add(MaxPooling2D(pool_size=(3, 3), strides=(
        1, 1), name="Conv4Pool"))  # Paper says (2,2) ?

    # Flattening
    model.add(Flatten())

    # FC #1
    model.add(Dense(units=5120, input_shape=(96,), activation='relu', ))
    model.add(ELU())
    model.add(Dropout(0.2))

    # FC #2
    model.add(Dense(units=96, input_shape=(2,), activation='relu'))

    # Output Layer
    model.add(Dense(units=n_output_classes, activation='softmax'))

    # Compile
    sgd_optimizer = SGD(lr=0.002, decay=0.1/350, momentum=1)
    model.compile(optimizer=sgd_optimizer,
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model


model = build_cracknet()
model.summary()


def train_cracknet(model,
                   target_size,
                   dataset_path,
                   training_path_prefix,
                   test_path_prefix,
                   history_file_path,
                   history_filename,
                   checkpoint_path,
                   checkpoint_prefix,
                   number_of_epochs,
                   tensorboard_log_path
                   ):

    train_datagen = ImageDataGenerator(rescale=1./255,
                                       shear_range=0.2,
                                       zoom_range=0.2,
                                       horizontal_flip=True)

    test_datagen = ImageDataGenerator(rescale=1./255)
    training_set_generator = train_datagen.flow_from_directory(
        dataset_path+training_path_prefix,
        target_size,
        batch_size=32,
        class_mode='categorical',
        shuffle=True,
        seed=42
    )
    test_set_generator = test_datagen.flow_from_directory(
        dataset_path+test_path_prefix,
        target_size,
        batch_size=32,
        class_mode='categorical',
        shuffle=True,
        seed=42
    )

    step_size_train = training_set_generator.n//training_set_generator.batch_size
    step_size_validation = test_set_generator.n//test_set_generator.batch_size

    check_pointer = ModelCheckpoint(
        checkpoint_path +
        '%s_weights.{epoch:02d}-{val_loss:.2f}.hdf5' % checkpoint_prefix,
        monitor='val_loss',
        mode='auto',
        save_best_only=True
    )

    tensorboard_logger = TensorBoard(
        log_dir=tensorboard_log_path, histogram_freq=0,
        write_graph=True, write_images=True
    )
    tensorboard_logger.set_model(model)

    csv_logger = CSVLogger(filename=history_file_path+history_filename)
    history = model.fit_generator(
        training_set_generator,
        steps_per_epoch=step_size_train,
        epochs=number_of_epochs,
        validation_data=test_set_generator,
        validation_steps=step_size_validation,
        callbacks=[check_pointer, csv_logger, tensorboard_logger]
    )
MODEL_NAME = "cracknet_cracks8020_v1"
train_cracknet(model,
                   target_size=(120, 120),
                   dataset_path=DATA_BASE_PATH+"/datasets/cracks_splitted8020/",
                   training_path_prefix="train_set",
                   test_path_prefix="test_set",
                   history_file_path=WORKSPACE_BASE_PATH+"/training_logs/",
                   history_filename=MODEL_NAME+".csv",
                   checkpoint_path=WORKSPACE_BASE_PATH+"/model-checkpoints/",
                   checkpoint_prefix=MODEL_NAME,
                   number_of_epochs=30,  # Original paper: 70 epochs / 32.535 seconds
                   tensorboard_log_path=WORKSPACE_BASE_PATH+"/tensorboard_logs/",
                   )


list_of_files = glob.glob(WORKSPACE_BASE_PATH+'/model-checkpoints/*.hdf5')
CHECKPOINT_FILE = max(list_of_files, key=os.path.getctime)  # last checkpoint
VERSION = 1
print(CHECKPOINT_FILE)

convert_from_keras_to_savedmodel(
    input_filename=CHECKPOINT_FILE,
    export_path=WORKSPACE_BASE_PATH+'/models/'+MODEL_NAME+"/"+str(VERSION)
)

